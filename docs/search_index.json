[["index.html", "datRtective Chapter 1 Data Wrangling in R 1.1 Remove NA values 1.2 Obtain the 5-number summary 1.3 Select columns 1.4 Method 1: use the index/name of the column 1.5 Filter rows 1.6 Pipes 1.7 Pipes 1.8 Group data by a criterion 1.9 Mutate", " datRtective Bao Maddux 2025-04-01 Chapter 1 Data Wrangling in R Manipulation of data frames is a common task when you start exploring your data in R and dplyr is a package for making tabular data manipulation easier. tidyverse is an “umbrella-package” that installs a series of packages useful for data analysis which work together well. Some of them are considered core packages (among them tidyr, dplyr, ggplot2), because you are likely to use them in almost every analysis. Other packages, like lubridate (to work with dates) or haven (for SPSS, Stata, and SAS data) that you are likely to use not for every analysis are also installed. 1.1 Remove NA values The problem of missing data is relatively common in almost all research and can have a significant effect on the conclusions that can be drawn from the data. The best possible method of handling the missing data is to prevent the problem by well-planning the study and collecting the data carefully. By far the most common approach to the missing data is to simply omit those cases with the missing data and analyze the remaining data. This approach is known as the complete case (or available case) analysis or listwise deletion. data(&quot;airquality&quot;) head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 newdata = na.omit(airquality) print(newdata) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 12 16 256 9.7 69 5 12 ## 13 11 290 9.2 66 5 13 ## 14 14 274 10.9 68 5 14 ## 15 18 65 13.2 58 5 15 ## 16 14 334 11.5 64 5 16 ## 17 34 307 12.0 66 5 17 ## 18 6 78 18.4 57 5 18 ## 19 30 322 11.5 68 5 19 ## 20 11 44 9.7 62 5 20 ## 21 1 8 9.7 59 5 21 ## 22 11 320 16.6 73 5 22 ## 23 4 25 9.7 61 5 23 ## 24 32 92 12.0 61 5 24 ## 28 23 13 12.0 67 5 28 ## 29 45 252 14.9 81 5 29 ## 30 115 223 5.7 79 5 30 ## 31 37 279 7.4 76 5 31 ## 38 29 127 9.7 82 6 7 ## 40 71 291 13.8 90 6 9 ## 41 39 323 11.5 87 6 10 ## 44 23 148 8.0 82 6 13 ## 47 21 191 14.9 77 6 16 ## 48 37 284 20.7 72 6 17 ## 49 20 37 9.2 65 6 18 ## 50 12 120 11.5 73 6 19 ## 51 13 137 10.3 76 6 20 ## 62 135 269 4.1 84 7 1 ## 63 49 248 9.2 85 7 2 ## 64 32 236 9.2 81 7 3 ## 66 64 175 4.6 83 7 5 ## 67 40 314 10.9 83 7 6 ## 68 77 276 5.1 88 7 7 ## 69 97 267 6.3 92 7 8 ## 70 97 272 5.7 92 7 9 ## 71 85 175 7.4 89 7 10 ## 73 10 264 14.3 73 7 12 ## 74 27 175 14.9 81 7 13 ## 76 7 48 14.3 80 7 15 ## 77 48 260 6.9 81 7 16 ## 78 35 274 10.3 82 7 17 ## 79 61 285 6.3 84 7 18 ## 80 79 187 5.1 87 7 19 ## 81 63 220 11.5 85 7 20 ## 82 16 7 6.9 74 7 21 ## 85 80 294 8.6 86 7 24 ## 86 108 223 8.0 85 7 25 ## 87 20 81 8.6 82 7 26 ## 88 52 82 12.0 86 7 27 ## 89 82 213 7.4 88 7 28 ## 90 50 275 7.4 86 7 29 ## 91 64 253 7.4 83 7 30 ## 92 59 254 9.2 81 7 31 ## 93 39 83 6.9 81 8 1 ## 94 9 24 13.8 81 8 2 ## 95 16 77 7.4 82 8 3 ## 99 122 255 4.0 89 8 7 ## 100 89 229 10.3 90 8 8 ## 101 110 207 8.0 90 8 9 ## 104 44 192 11.5 86 8 12 ## 105 28 273 11.5 82 8 13 ## 106 65 157 9.7 80 8 14 ## 108 22 71 10.3 77 8 16 ## 109 59 51 6.3 79 8 17 ## 110 23 115 7.4 76 8 18 ## 111 31 244 10.9 78 8 19 ## 112 44 190 10.3 78 8 20 ## 113 21 259 15.5 77 8 21 ## 114 9 36 14.3 72 8 22 ## 116 45 212 9.7 79 8 24 ## 117 168 238 3.4 81 8 25 ## 118 73 215 8.0 86 8 26 ## 120 76 203 9.7 97 8 28 ## 121 118 225 2.3 94 8 29 ## 122 84 237 6.3 96 8 30 ## 123 85 188 6.3 94 8 31 ## 124 96 167 6.9 91 9 1 ## 125 78 197 5.1 92 9 2 ## 126 73 183 2.8 93 9 3 ## 127 91 189 4.6 93 9 4 ## 128 47 95 7.4 87 9 5 ## 129 32 92 15.5 84 9 6 ## 130 20 252 10.9 80 9 7 ## 131 23 220 10.3 78 9 8 ## 132 21 230 10.9 75 9 9 ## 133 24 259 9.7 73 9 10 ## 134 44 236 14.9 81 9 11 ## 135 21 259 15.5 76 9 12 ## 136 28 238 6.3 77 9 13 ## 137 9 24 10.9 71 9 14 ## 138 13 112 11.5 71 9 15 ## 139 46 237 6.9 78 9 16 ## 140 18 224 13.8 67 9 17 ## 141 13 27 10.3 76 9 18 ## 142 24 238 10.3 68 9 19 ## 143 16 201 8.0 82 9 20 ## 144 13 238 12.6 64 9 21 ## 145 23 14 9.2 71 9 22 ## 146 36 139 10.3 81 9 23 ## 147 7 49 10.3 69 9 24 ## 148 14 20 16.6 63 9 25 ## 149 30 193 6.9 70 9 26 ## 151 14 191 14.3 75 9 28 ## 152 18 131 8.0 76 9 29 ## 153 20 223 11.5 68 9 30 1.2 Obtain the 5-number summary data(&#39;women&#39;) summary(women) ## height weight ## Min. :58.0 Min. :115.0 ## 1st Qu.:61.5 1st Qu.:124.5 ## Median :65.0 Median :135.0 ## Mean :65.0 Mean :136.7 ## 3rd Qu.:68.5 3rd Qu.:148.0 ## Max. :72.0 Max. :164.0 1.3 Select columns 1.4 Method 1: use the index/name of the column To extract the first column in women data set, one could insert the index of the column, which is 1, in between the square brackets, following the name of the data set. women[1] ## height ## 1 58 ## 2 59 ## 3 60 ## 4 61 ## 5 62 ## 6 63 ## 7 64 ## 8 65 ## 9 66 ## 10 67 ## 11 68 ## 12 69 ## 13 70 ## 14 71 ## 15 72 Otherwise, one could also use the name of the column, in replacement of the its index, to extract the column from the data frame. women[&quot;height&quot;] ## height ## 1 58 ## 2 59 ## 3 60 ## 4 61 ## 5 62 ## 6 63 ## 7 64 ## 8 65 ## 9 66 ## 10 67 ## 11 68 ## 12 69 ## 13 70 ## 14 71 ## 15 72 To select multiple columns, a vector including the names of the desired columns needs passing in between the square brackets as follows. women[c(&quot;height&quot;,&quot;weight&quot;)] ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 1.4.1 Method 2: select from tidyverse package The first argument of the select() command is the data frame you’d like to extract data from, and the subsequent arguments are the name of the columns you wish to keep. library(tidyverse) select(women,height) ## height ## 1 58 ## 2 59 ## 3 60 ## 4 61 ## 5 62 ## 6 63 ## 7 64 ## 8 65 ## 9 66 ## 10 67 ## 11 68 ## 12 69 ## 13 70 ## 14 71 ## 15 72 In the event that more than one columns are of interest, one can list other column names after the first column, separated by a comma. select(women,height,weight) ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 1.4.2 Method 3: subset() Alternatively, you can also carry out the same task by using subset() command. The first argument in subset() is the data frame to be subsetted, followed by a logical expression indicating elements or rows to keep, and subsequently, an expression to indicate which columns to select from the original data frame. subset(women,select = height) ## height ## 1 58 ## 2 59 ## 3 60 ## 4 61 ## 5 62 ## 6 63 ## 7 64 ## 8 65 ## 9 66 ## 10 67 ## 11 68 ## 12 69 ## 13 70 ## 14 71 ## 15 72 To select more than one columns, one can pass a vector including the column names into select argument in subset(). subset(women,select = c(height,weight)) ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 ## 7 64 132 ## 8 65 135 ## 9 66 139 ## 10 67 142 ## 11 68 146 ## 12 69 150 ## 13 70 154 ## 14 71 159 ## 15 72 164 1.5 Filter rows 1.5.1 Method 1: filter() from tidyverse package data(&quot;airquality&quot;) filter(airquality,Month==5 &amp; Temp &gt;60) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 28 NA 14.9 66 5 6 ## 6 23 299 8.6 65 5 7 ## 7 8 19 20.1 61 5 9 ## 8 NA 194 8.6 69 5 10 ## 9 7 NA 6.9 74 5 11 ## 10 16 256 9.7 69 5 12 ## 11 11 290 9.2 66 5 13 ## 12 14 274 10.9 68 5 14 ## 13 14 334 11.5 64 5 16 ## 14 34 307 12.0 66 5 17 ## 15 30 322 11.5 68 5 19 ## 16 11 44 9.7 62 5 20 ## 17 11 320 16.6 73 5 22 ## 18 4 25 9.7 61 5 23 ## 19 32 92 12.0 61 5 24 ## 20 23 13 12.0 67 5 28 ## 21 45 252 14.9 81 5 29 ## 22 115 223 5.7 79 5 30 ## 23 37 279 7.4 76 5 31 1.5.2 Method 2: subset() data(&quot;airquality&quot;) subset(airquality,Month==5 &amp; Temp &gt;60) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 6 28 NA 14.9 66 5 6 ## 7 23 299 8.6 65 5 7 ## 9 8 19 20.1 61 5 9 ## 10 NA 194 8.6 69 5 10 ## 11 7 NA 6.9 74 5 11 ## 12 16 256 9.7 69 5 12 ## 13 11 290 9.2 66 5 13 ## 14 14 274 10.9 68 5 14 ## 16 14 334 11.5 64 5 16 ## 17 34 307 12.0 66 5 17 ## 19 30 322 11.5 68 5 19 ## 20 11 44 9.7 62 5 20 ## 22 11 320 16.6 73 5 22 ## 23 4 25 9.7 61 5 23 ## 24 32 92 12.0 61 5 24 ## 28 23 13 12.0 67 5 28 ## 29 45 252 14.9 81 5 29 ## 30 115 223 5.7 79 5 30 ## 31 37 279 7.4 76 5 31 1.6 Pipes What if you wished to perform multiple consecutive procedures on the same data? For example, in the airquality data set, let’s construct the 5-number summary the Wind column of days that are in May and have temperature above 60. There are three primary approaches to this problem: use intermediate steps, nested functions, or pipes. 1.6.1 Intermediate steps First and foremost, we will want to impose the restriction of month and temperature on the original data set by using filter(), assign this new data set a name, apply select() on it and lastly, use summary to construct the 5-number summary. new = filter(airquality,Month==5 &amp; Temp &gt;60) wind = select(new,Wind) summary(wind) ## Wind ## Min. : 5.70 ## 1st Qu.: 8.60 ## Median :10.90 ## Mean :10.93 ## 3rd Qu.:12.00 ## Max. :20.10 This is easy to interpret, but can clutter up your workspace with lots of objects that you have to name individually. 1.6.2 Nested functions summary(select(filter(airquality,Month==5 &amp; Temp &gt;60),Wind)) ## Wind ## Min. : 5.70 ## 1st Qu.: 8.60 ## Median :10.90 ## Mean :10.93 ## 3rd Qu.:12.00 ## Max. :20.10 This second approach, on the other hand, appears more complicated to read as the problem involves more steps. 1.7 Pipes Pipes allow you to feed an object forward to a function or call expression, thereby allowing you to express a sequence of operations that transform an object. By default, the pipe passes the object on its left-hand side to the first argument of the function on the right-hand side. airquality %&gt;% filter(Month==5 &amp; Temp &gt;60) %&gt;% select(Wind) %&gt;% summary ## Wind ## Min. : 5.70 ## 1st Qu.: 8.60 ## Median :10.90 ## Mean :10.93 ## 3rd Qu.:12.00 ## Max. :20.10 1.8 Group data by a criterion summarise() creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified. airquality %&gt;% group_by(Month) %&gt;% summarise(Average = mean(Temp)) ## # A tibble: 5 × 2 ## Month Average ## &lt;int&gt; &lt;dbl&gt; ## 1 5 65.5 ## 2 6 79.1 ## 3 7 83.9 ## 4 8 84.0 ## 5 9 76.9 1.9 Mutate airquality %&gt;% mutate(Hot = ifelse(Temp &gt; 70, &quot;Yes&quot;,&quot;No&quot;)) ## Ozone Solar.R Wind Temp Month Day Hot ## 1 41 190 7.4 67 5 1 No ## 2 36 118 8.0 72 5 2 Yes ## 3 12 149 12.6 74 5 3 Yes ## 4 18 313 11.5 62 5 4 No ## 5 NA NA 14.3 56 5 5 No ## 6 28 NA 14.9 66 5 6 No ## 7 23 299 8.6 65 5 7 No ## 8 19 99 13.8 59 5 8 No ## 9 8 19 20.1 61 5 9 No ## 10 NA 194 8.6 69 5 10 No ## 11 7 NA 6.9 74 5 11 Yes ## 12 16 256 9.7 69 5 12 No ## 13 11 290 9.2 66 5 13 No ## 14 14 274 10.9 68 5 14 No ## 15 18 65 13.2 58 5 15 No ## 16 14 334 11.5 64 5 16 No ## 17 34 307 12.0 66 5 17 No ## 18 6 78 18.4 57 5 18 No ## 19 30 322 11.5 68 5 19 No ## 20 11 44 9.7 62 5 20 No ## 21 1 8 9.7 59 5 21 No ## 22 11 320 16.6 73 5 22 Yes ## 23 4 25 9.7 61 5 23 No ## 24 32 92 12.0 61 5 24 No ## 25 NA 66 16.6 57 5 25 No ## 26 NA 266 14.9 58 5 26 No ## 27 NA NA 8.0 57 5 27 No ## 28 23 13 12.0 67 5 28 No ## 29 45 252 14.9 81 5 29 Yes ## 30 115 223 5.7 79 5 30 Yes ## 31 37 279 7.4 76 5 31 Yes ## 32 NA 286 8.6 78 6 1 Yes ## 33 NA 287 9.7 74 6 2 Yes ## 34 NA 242 16.1 67 6 3 No ## 35 NA 186 9.2 84 6 4 Yes ## 36 NA 220 8.6 85 6 5 Yes ## 37 NA 264 14.3 79 6 6 Yes ## 38 29 127 9.7 82 6 7 Yes ## 39 NA 273 6.9 87 6 8 Yes ## 40 71 291 13.8 90 6 9 Yes ## 41 39 323 11.5 87 6 10 Yes ## 42 NA 259 10.9 93 6 11 Yes ## 43 NA 250 9.2 92 6 12 Yes ## 44 23 148 8.0 82 6 13 Yes ## 45 NA 332 13.8 80 6 14 Yes ## 46 NA 322 11.5 79 6 15 Yes ## 47 21 191 14.9 77 6 16 Yes ## 48 37 284 20.7 72 6 17 Yes ## 49 20 37 9.2 65 6 18 No ## 50 12 120 11.5 73 6 19 Yes ## 51 13 137 10.3 76 6 20 Yes ## 52 NA 150 6.3 77 6 21 Yes ## 53 NA 59 1.7 76 6 22 Yes ## 54 NA 91 4.6 76 6 23 Yes ## 55 NA 250 6.3 76 6 24 Yes ## 56 NA 135 8.0 75 6 25 Yes ## 57 NA 127 8.0 78 6 26 Yes ## 58 NA 47 10.3 73 6 27 Yes ## 59 NA 98 11.5 80 6 28 Yes ## 60 NA 31 14.9 77 6 29 Yes ## 61 NA 138 8.0 83 6 30 Yes ## 62 135 269 4.1 84 7 1 Yes ## 63 49 248 9.2 85 7 2 Yes ## 64 32 236 9.2 81 7 3 Yes ## 65 NA 101 10.9 84 7 4 Yes ## 66 64 175 4.6 83 7 5 Yes ## 67 40 314 10.9 83 7 6 Yes ## 68 77 276 5.1 88 7 7 Yes ## 69 97 267 6.3 92 7 8 Yes ## 70 97 272 5.7 92 7 9 Yes ## 71 85 175 7.4 89 7 10 Yes ## 72 NA 139 8.6 82 7 11 Yes ## 73 10 264 14.3 73 7 12 Yes ## 74 27 175 14.9 81 7 13 Yes ## 75 NA 291 14.9 91 7 14 Yes ## 76 7 48 14.3 80 7 15 Yes ## 77 48 260 6.9 81 7 16 Yes ## 78 35 274 10.3 82 7 17 Yes ## 79 61 285 6.3 84 7 18 Yes ## 80 79 187 5.1 87 7 19 Yes ## 81 63 220 11.5 85 7 20 Yes ## 82 16 7 6.9 74 7 21 Yes ## 83 NA 258 9.7 81 7 22 Yes ## 84 NA 295 11.5 82 7 23 Yes ## 85 80 294 8.6 86 7 24 Yes ## 86 108 223 8.0 85 7 25 Yes ## 87 20 81 8.6 82 7 26 Yes ## 88 52 82 12.0 86 7 27 Yes ## 89 82 213 7.4 88 7 28 Yes ## 90 50 275 7.4 86 7 29 Yes ## 91 64 253 7.4 83 7 30 Yes ## 92 59 254 9.2 81 7 31 Yes ## 93 39 83 6.9 81 8 1 Yes ## 94 9 24 13.8 81 8 2 Yes ## 95 16 77 7.4 82 8 3 Yes ## 96 78 NA 6.9 86 8 4 Yes ## 97 35 NA 7.4 85 8 5 Yes ## 98 66 NA 4.6 87 8 6 Yes ## 99 122 255 4.0 89 8 7 Yes ## 100 89 229 10.3 90 8 8 Yes ## 101 110 207 8.0 90 8 9 Yes ## 102 NA 222 8.6 92 8 10 Yes ## 103 NA 137 11.5 86 8 11 Yes ## 104 44 192 11.5 86 8 12 Yes ## 105 28 273 11.5 82 8 13 Yes ## 106 65 157 9.7 80 8 14 Yes ## 107 NA 64 11.5 79 8 15 Yes ## 108 22 71 10.3 77 8 16 Yes ## 109 59 51 6.3 79 8 17 Yes ## 110 23 115 7.4 76 8 18 Yes ## 111 31 244 10.9 78 8 19 Yes ## 112 44 190 10.3 78 8 20 Yes ## 113 21 259 15.5 77 8 21 Yes ## 114 9 36 14.3 72 8 22 Yes ## 115 NA 255 12.6 75 8 23 Yes ## 116 45 212 9.7 79 8 24 Yes ## 117 168 238 3.4 81 8 25 Yes ## 118 73 215 8.0 86 8 26 Yes ## 119 NA 153 5.7 88 8 27 Yes ## 120 76 203 9.7 97 8 28 Yes ## 121 118 225 2.3 94 8 29 Yes ## 122 84 237 6.3 96 8 30 Yes ## 123 85 188 6.3 94 8 31 Yes ## 124 96 167 6.9 91 9 1 Yes ## 125 78 197 5.1 92 9 2 Yes ## 126 73 183 2.8 93 9 3 Yes ## 127 91 189 4.6 93 9 4 Yes ## 128 47 95 7.4 87 9 5 Yes ## 129 32 92 15.5 84 9 6 Yes ## 130 20 252 10.9 80 9 7 Yes ## 131 23 220 10.3 78 9 8 Yes ## 132 21 230 10.9 75 9 9 Yes ## 133 24 259 9.7 73 9 10 Yes ## 134 44 236 14.9 81 9 11 Yes ## 135 21 259 15.5 76 9 12 Yes ## 136 28 238 6.3 77 9 13 Yes ## 137 9 24 10.9 71 9 14 Yes ## 138 13 112 11.5 71 9 15 Yes ## 139 46 237 6.9 78 9 16 Yes ## 140 18 224 13.8 67 9 17 No ## 141 13 27 10.3 76 9 18 Yes ## 142 24 238 10.3 68 9 19 No ## 143 16 201 8.0 82 9 20 Yes ## 144 13 238 12.6 64 9 21 No ## 145 23 14 9.2 71 9 22 Yes ## 146 36 139 10.3 81 9 23 Yes ## 147 7 49 10.3 69 9 24 No ## 148 14 20 16.6 63 9 25 No ## 149 30 193 6.9 70 9 26 No ## 150 NA 145 13.2 77 9 27 Yes ## 151 14 191 14.3 75 9 28 Yes ## 152 18 131 8.0 76 9 29 Yes ## 153 20 223 11.5 68 9 30 No airquality %&gt;% mutate(Hot = ifelse(Temp &gt; 70, &quot;Yes&quot;,&quot;No&quot;)) %&gt;% group_by(Hot) %&gt;% summarise(&quot;Average Temp&quot; = mean(Temp)) ## # A tibble: 2 × 2 ## Hot `Average Temp` ## &lt;chr&gt; &lt;dbl&gt; ## 1 No 63.8 ## 2 Yes 81.8 "],["how-to-ruin-a-regression-model.html", "Chapter 2 How to Ruin a Regression Model 2.1 Forgot to Check Linear Regression Assumptions", " Chapter 2 How to Ruin a Regression Model 2.1 Forgot to Check Linear Regression Assumptions Linearity. This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. Note that this assumption puts restriction on the parameters only and the predictor variables are treated as fixed values (see above). The predictor variables themselves can be arbitrarily transformed. How to check: One can verify this assumption visually by a scatterplot of the response variable against the predictor variable, or numerically by the correlation coefficient. However, in the event of multiple predictor variables, individual correlation coefficients or individual scatterplots are no longer feasible. The most common way to check linearity in this case is to scatter-plot residuals against the linearly predicted values. Curved or non-horizontally spead cloud on such a plot is diagnostic for non-linearity. head(women) ## height weight ## 1 58 115 ## 2 59 117 ## 3 60 120 ## 4 61 123 ## 5 62 126 ## 6 63 129 plot(women$height,women$weight,xlab=&quot;Women&#39;s Heights&quot;,ylab=&quot;Women&#39;s Weights&quot;, main=&quot;Scatterplot of Women&#39;s Heights against their Weights&quot;) cor(women$height,women$weight) ## [1] 0.9954948 In the next example, we will examine data set Carseats from ISLR library. There are 2 ways to assess a function from a particular library in R. One can either conjure up the library in advance library(ISLR) ## Warning: package &#39;ISLR&#39; was built under R version 4.3.3 head(Carseats) ## Sales CompPrice Income Advertising Population Price ShelveLoc Age Education ## 1 9.50 138 73 11 276 120 Bad 42 17 ## 2 11.22 111 48 16 260 83 Good 65 10 ## 3 10.06 113 35 10 269 80 Medium 59 12 ## 4 7.40 117 100 4 466 97 Medium 55 14 ## 5 4.15 141 64 3 340 128 Bad 38 13 ## 6 10.81 124 113 13 501 72 Bad 78 16 ## Urban US ## 1 Yes Yes ## 2 Yes Yes ## 3 Yes Yes ## 4 Yes Yes ## 5 Yes No ## 6 No Yes or one can also use double colons (::) to explicitly specify the library head(ISLR::Carseats) ## Sales CompPrice Income Advertising Population Price ShelveLoc Age Education ## 1 9.50 138 73 11 276 120 Bad 42 17 ## 2 11.22 111 48 16 260 83 Good 65 10 ## 3 10.06 113 35 10 269 80 Medium 59 12 ## 4 7.40 117 100 4 466 97 Medium 55 14 ## 5 4.15 141 64 3 340 128 Bad 38 13 ## 6 10.81 124 113 13 501 72 Bad 78 16 ## Urban US ## 1 Yes Yes ## 2 Yes Yes ## 3 Yes Yes ## 4 Yes Yes ## 5 Yes No ## 6 No Yes To investigate the correlation among quantitative variables in this data set, let use eliminate qualitative variables (ShelveLoc, Urban, US) from the original data set and rename the altered data set to be newdat. One can get rid of the columns corresponding to aforementioned qualitative variables by their indices. newdat = ISLR::Carseats[,-c(7,10,11)] Alternatively, the task can also be done by employing function subset and the column names as follows newdat = subset(ISLR::Carseats,select=-c(ShelveLoc, Urban, US)) Passing newdat into cor function, cor(newdat), one will receive a correlation matrix for all possible pairs of variables in newdat. library(corrplot) ## Warning: package &#39;corrplot&#39; was built under R version 4.3.3 ## corrplot 0.95 loaded cormat = cor(newdat) print(cormat) ## Sales CompPrice Income Advertising Population ## Sales 1.00000000 0.06407873 0.151950979 0.269506781 0.050470984 ## CompPrice 0.06407873 1.00000000 -0.080653423 -0.024198788 -0.094706516 ## Income 0.15195098 -0.08065342 1.000000000 0.058994706 -0.007876994 ## Advertising 0.26950678 -0.02419879 0.058994706 1.000000000 0.265652145 ## Population 0.05047098 -0.09470652 -0.007876994 0.265652145 1.000000000 ## Price -0.44495073 0.58484777 -0.056698202 0.044536874 -0.012143620 ## Age -0.23181544 -0.10023882 -0.004670094 -0.004557497 -0.042663355 ## Education -0.05195524 0.02519705 -0.056855422 -0.033594307 -0.106378231 ## Price Age Education ## Sales -0.44495073 -0.231815440 -0.051955242 ## CompPrice 0.58484777 -0.100238817 0.025197050 ## Income -0.05669820 -0.004670094 -0.056855422 ## Advertising 0.04453687 -0.004557497 -0.033594307 ## Population -0.01214362 -0.042663355 -0.106378231 ## Price 1.00000000 -0.102176839 0.011746599 ## Age -0.10217684 1.000000000 0.006488032 ## Education 0.01174660 0.006488032 1.000000000 corrplot(cormat) Constant variance (homoscedasticity). This means that the variance of the errors does not depend on the values of the predictor variables. Thus the variability of the responses for given fixed values of the predictors is the same regardless of how large or small the responses are. The presence of heteroscedasticity will result in an overall “average” estimate of variance being used instead of one that takes into account the true variance structure. This leads to less precise (but in the case of ordinary least squares, not biased) parameter estimates and biased standard errors, resulting in misleading tests and interval estimates. The mean squared error for the model will also be wrong. How to check: Graphically, this assumption can be validated by a plot of residuals versus predicted values (or the values of each individual predictor) can be examined for a “tunnel effect” (i.e. increasing or decreasing vertical spread as one moves left to right on the plot). Formally, Breusch-Pagan test (or Koenker-Bassett test) can be used to assess this assumption. Bear in mind that the Breusch-Pagan test tends to be sensitive to departures from normality or small sample sizes. data(&quot;women&quot;) model1 = lm(formula = women$height~ women$weight, data = women) summary(model1) ## ## Call: ## lm(formula = women$height ~ women$weight, data = women) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.83233 -0.26249 0.08314 0.34353 0.49790 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.723456 1.043746 24.64 2.68e-12 *** ## women$weight 0.287249 0.007588 37.85 1.09e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.44 on 13 degrees of freedom ## Multiple R-squared: 0.991, Adjusted R-squared: 0.9903 ## F-statistic: 1433 on 1 and 13 DF, p-value: 1.091e-14 plot(model1$fitted.values,resid(model1),xlab=&quot;Fitted Values&quot;,ylab=&quot;Residuals&quot;, main=&quot;Residuals versus Fitted Values Plot&quot;) abline(0,0) There is a conspicuous pattern in residuals for the model \\(\\hat{height} = a + b\\times weight\\). The curvilinear trajectory, as a matter of fact, suggests a quadratic relationship between womens’ heights and their weights. It is reasonable to add a second order term into the model. data(&quot;women&quot;) model2 = lm(formula = women$height~ women$weight + I(women$weight^2), data = women) summary(model2) ## ## Call: ## lm(formula = women$height ~ women$weight + I(women$weight^2), ## data = women) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.105338 -0.035764 -0.004898 0.049430 0.141593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.175e+01 1.720e+00 -6.83 1.82e-05 *** ## women$weight 8.343e-01 2.502e-02 33.35 3.36e-13 *** ## I(women$weight^2) -1.973e-03 9.014e-05 -21.89 4.84e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07158 on 12 degrees of freedom ## Multiple R-squared: 0.9998, Adjusted R-squared: 0.9997 ## F-statistic: 2.732e+04 on 2 and 12 DF, p-value: &lt; 2.2e-16 plot(model2$fitted.values,resid(model2),xlab=&quot;Fitted Values&quot;,ylab=&quot;Residuals&quot;, main=&quot;Residuals versus Fitted Values Plot (Model 2)&quot;) abline(0,0) Independence of errors. This assumes that the errors of the response variables are uncorrelated with each other. How to check: To test for violations of independence, you can look at plots of the residuals versus predictor variables or plots of residuals versus row number in situations where the rows have been sorted or grouped in some way that depends (only) on the values of the predictor variables. The residuals should be randomly and symmetrically distributed around zero. Normally distributed errors implies that the errors follow a Normal distribution. Technically, the normal distribution assumption is not necessary if you are willing to assume the model equation is correct and your only goal is to estimate its coefficients and generate predictions in such a way as to minimize mean squared error. The formulas for estimating coefficients require no more than that, and some references on regression analysis do not list normally distributed errors among the key assumptions. But generally we are interested in making inferences about the model and/or estimating the probability that a given forecast error will exceed some threshold in a particular direction, in which case distributional assumptions are important. Also, a significant violation of the normal distribution assumption is often a “red flag” indicating that there is some other problem with the model assumptions and/or that there are a few unusual data points that should be studied closely and/or that a better model is still waiting out there somewhere. data(&quot;women&quot;) model = lm(formula = women$height~ women$weight, data = women) summary(model) ## ## Call: ## lm(formula = women$height ~ women$weight, data = women) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.83233 -0.26249 0.08314 0.34353 0.49790 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.723456 1.043746 24.64 2.68e-12 *** ## women$weight 0.287249 0.007588 37.85 1.09e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.44 on 13 degrees of freedom ## Multiple R-squared: 0.991, Adjusted R-squared: 0.9903 ## F-statistic: 1433 on 1 and 13 DF, p-value: 1.091e-14 res = resid(model) qqnorm(res) qqline(res) data(&quot;women&quot;) model = lm(formula = women$height~ women$weight + I(women$weight^2), data = women) summary(model) ## ## Call: ## lm(formula = women$height ~ women$weight + I(women$weight^2), ## data = women) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.105338 -0.035764 -0.004898 0.049430 0.141593 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.175e+01 1.720e+00 -6.83 1.82e-05 *** ## women$weight 8.343e-01 2.502e-02 33.35 3.36e-13 *** ## I(women$weight^2) -1.973e-03 9.014e-05 -21.89 4.84e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.07158 on 12 degrees of freedom ## Multiple R-squared: 0.9998, Adjusted R-squared: 0.9997 ## F-statistic: 2.732e+04 on 2 and 12 DF, p-value: &lt; 2.2e-16 res = resid(model) qqnorm(res) qqline(res) Lack of perfect multicollinearity in the predictors. Perfect multicollinearity refers to a situation where the predictive variables have an exact linear relationship. This can be caused by accidentally duplicating a variable in the data, using a linear transformation of a variable along with the original (e.g., the same temperature measurements expressed in Fahrenheit and Celsius), or including a linear combination of multiple variables in the model, such as their mean. "],["the-pool-of-tears.html", "Chapter 3 The pool of tears", " Chapter 3 The pool of tears What the hell is “the pool of tears”?? "],["a-caucus-race-and-a-long-tale.html", "Chapter 4 A caucus-race and a long tale", " Chapter 4 A caucus-race and a long tale "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
